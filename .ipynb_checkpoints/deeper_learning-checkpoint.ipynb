{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm,trange\n",
    "from Bio.Seq import *\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ROOT_DIR - root directory\n",
    "ROOT_DIR = os.getcwd()+'/'\n",
    "\n",
    "# FEATURE_DIR - directory where feature dataframes are saved\n",
    "DATA_DIR = ROOT_DIR + 'dataframes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45206\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prest_id</th>\n",
       "      <th>uniprot_id</th>\n",
       "      <th>conc_cf</th>\n",
       "      <th>aa_seq</th>\n",
       "      <th>nt_seq</th>\n",
       "      <th>aa_len</th>\n",
       "      <th>true_nt_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140095</td>\n",
       "      <td>G3V3N0</td>\n",
       "      <td>4.3075</td>\n",
       "      <td>IMTAPSSFEQFKVAMNYLQLYNVPDCLEDIQDADCSSSKCSSSASS...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCAATTATGACAGCTCCCTCCAGTTTTGAGC...</td>\n",
       "      <td>139</td>\n",
       "      <td>ATTATGACAGCTCCCTCCAGTTTTGAGCAGTTTAAAGTGGCAATGA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>140099</td>\n",
       "      <td>G3V537</td>\n",
       "      <td>2.9154</td>\n",
       "      <td>TYYAWKHELLGSGTCPALPPREVLGMEELEKLPEEQVAEEELECSA...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCAACCTACTATGCCTGGAAGCATGAGCTGC...</td>\n",
       "      <td>144</td>\n",
       "      <td>ACCTACTATGCCTGGAAGCATGAGCTGCTGGGCTCTGGCACCTGCC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>140225</td>\n",
       "      <td>P12724</td>\n",
       "      <td>1.4877</td>\n",
       "      <td>SLHARPPQFTRAQWFAIQHISLNPPRCTIAMRAINNYRWRCKNQNT...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCATCACTCCATGCCAGACCCCCACAGTTTA...</td>\n",
       "      <td>136</td>\n",
       "      <td>TCACTCCATGCCAGACCCCCACAGTTTACGAGGGCTCAGTGGTTTG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>140235</td>\n",
       "      <td>H0YH02</td>\n",
       "      <td>6.7224</td>\n",
       "      <td>ARALNESKRVNNGNTAPEDSSPAKKTRRCQRQESKKMPVAGGKANK...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCAGCGAGAGCATTAAATGAAAGCAAAAGAG...</td>\n",
       "      <td>123</td>\n",
       "      <td>GCGAGAGCATTAAATGAAAGCAAAAGAGTTAATAATGGCAACACGG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>140309</td>\n",
       "      <td>F5GYC5</td>\n",
       "      <td>3.3848</td>\n",
       "      <td>HRKEPGARLEATRGAARPHKQGTKPMITRPSVSQLGEGKCPSSQHL...</td>\n",
       "      <td>GACAAGCTTGCGGCCGCACATCGGAAAGAGCCTGGGGCAAGGCTGG...</td>\n",
       "      <td>124</td>\n",
       "      <td>CATCGGAAAGAGCCTGGGGCAAGGCTGGAGGCCACAAGAGGAGCTG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   prest_id uniprot_id  conc_cf  \\\n",
       "0    140095     G3V3N0   4.3075   \n",
       "1    140099     G3V537   2.9154   \n",
       "2    140225     P12724   1.4877   \n",
       "3    140235     H0YH02   6.7224   \n",
       "4    140309     F5GYC5   3.3848   \n",
       "\n",
       "                                              aa_seq  \\\n",
       "0  IMTAPSSFEQFKVAMNYLQLYNVPDCLEDIQDADCSSSKCSSSASS...   \n",
       "1  TYYAWKHELLGSGTCPALPPREVLGMEELEKLPEEQVAEEELECSA...   \n",
       "2  SLHARPPQFTRAQWFAIQHISLNPPRCTIAMRAINNYRWRCKNQNT...   \n",
       "3  ARALNESKRVNNGNTAPEDSSPAKKTRRCQRQESKKMPVAGGKANK...   \n",
       "4  HRKEPGARLEATRGAARPHKQGTKPMITRPSVSQLGEGKCPSSQHL...   \n",
       "\n",
       "                                              nt_seq  aa_len  \\\n",
       "0  GACAAGCTTGCGGCCGCAATTATGACAGCTCCCTCCAGTTTTGAGC...     139   \n",
       "1  GACAAGCTTGCGGCCGCAACCTACTATGCCTGGAAGCATGAGCTGC...     144   \n",
       "2  GACAAGCTTGCGGCCGCATCACTCCATGCCAGACCCCCACAGTTTA...     136   \n",
       "3  GACAAGCTTGCGGCCGCAGCGAGAGCATTAAATGAAAGCAAAAGAG...     123   \n",
       "4  GACAAGCTTGCGGCCGCACATCGGAAAGAGCCTGGGGCAAGGCTGG...     124   \n",
       "\n",
       "                                         true_nt_seq  \n",
       "0  ATTATGACAGCTCCCTCCAGTTTTGAGCAGTTTAAAGTGGCAATGA...  \n",
       "1  ACCTACTATGCCTGGAAGCATGAGCTGCTGGGCTCTGGCACCTGCC...  \n",
       "2  TCACTCCATGCCAGACCCCCACAGTTTACGAGGGCTCAGTGGTTTG...  \n",
       "3  GCGAGAGCATTAAATGAAAGCAAAAGAGTTAATAATGGCAACACGG...  \n",
       "4  CATCGGAAAGAGCCTGGGGCAAGGCTGGAGGCCACAAGAGGAGCTG...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF_prest=pd.read_csv(DATA_DIR+'DF_prest.csv',index_col=0)\n",
    "print(len(DF_prest))\n",
    "DF_prest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'expressed'\n",
    "DF_prest.loc[:,target] = DF_prest.conc_cf > DF_prest.conc_cf.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number expressed: 11301\n",
      "Number not expressed: 11302\n",
      "Min expression cutoff: 7.2576\n",
      "Max non-expression: 3.269225\n"
     ]
    }
   ],
   "source": [
    "min_expressed = np.percentile(DF_prest['conc_cf'],75)\n",
    "max_not_expressed = np.percentile(DF_prest['conc_cf'],25)\n",
    "\n",
    "DF_prest_filtered = DF_prest[(DF_prest['conc_cf'] > min_expressed) | (DF_prest['conc_cf'] < max_not_expressed)]\n",
    "\n",
    "print('Number expressed:',len(DF_prest_filtered[DF_prest_filtered[target]]))\n",
    "print('Number not expressed:',len(DF_prest_filtered[DF_prest_filtered[target]==False]))\n",
    "print('Min expression cutoff:',min_expressed)\n",
    "print('Max non-expression:',max_not_expressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_inds = random.sample(DF_prest_filtered.index,int(len(DF_prest_filtered)*.6))\n",
    "test_inds = random.sample(set(DF_prest_filtered.index) - set(train_inds), int(len(DF_prest_filtered)*.3))\n",
    "val_inds = set(DF_prest_filtered.index) - set(train_inds) - set(test_inds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test-Train-Val Ratio: 0.6 - 0.3 - 0.1\n"
     ]
    }
   ],
   "source": [
    "print('Test-Train-Val Ratio: %.1f - %.1f - %.1f'%(len(train_inds)/len(DF_prest_filtered),\n",
    "                                                  len(test_inds)/len(DF_prest_filtered),\n",
    "                                                  len(val_inds)/len(DF_prest_filtered)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_vals = DF_prest_filtered.loc[train_inds,'expressed'].values.astype(np.float32).reshape((len(train_inds),1))\n",
    "test_vals = DF_prest_filtered.loc[test_inds,'expressed'].values.astype(np.float32).reshape((len(test_inds),1))\n",
    "val_vals = DF_prest_filtered.loc[val_inds,'expressed'].values.astype(np.float32).reshape((len(val_inds),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode nucleotide sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "onehot_nuc = {'A':[1,0,0,0],'G':[0,1,0,0],'C':[0,0,1,0],'T':[0,0,0,1]}\n",
    "\n",
    "def encode(seq,padlength=0):\n",
    "    raw_encoding = np.array([onehot_nuc[char] for char in list(seq)],dtype=np.float32)\n",
    "    return np.vstack([raw_encoding,np.zeros((padlength - len(seq),4),dtype=np.float32)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = int(np.ceil(max([len(seq) for seq in DF_prest_filtered.nt_seq])/4)*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoded_train = np.vstack(([encode(seq,maxlen).flatten() for seq in DF_prest_filtered.loc[train_inds,'nt_seq']]))\n",
    "encoded_test = np.vstack(([encode(seq,maxlen).flatten() for seq in DF_prest_filtered.loc[test_inds,'nt_seq']]))\n",
    "encoded_val = np.vstack(([encode(seq,maxlen).flatten() for seq in DF_prest_filtered.loc[val_inds,'nt_seq']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13561, 2256)\n",
      "(6780, 2256)\n",
      "(2262, 2256)\n"
     ]
    }
   ],
   "source": [
    "print(encoded_train.shape)\n",
    "print(encoded_test.shape)\n",
    "print(encoded_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set = tf.contrib.learn.datasets.base.Dataset(encoded_train,train_vals)\n",
    "test_set = tf.contrib.learn.datasets.base.Dataset(encoded_test,test_vals)\n",
    "val_set = tf.contrib.learn.datasets.base.Dataset(encoded_val,val_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = tf.contrib.learn.datasets.base.Datasets(train_set,test_set,val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_iters = 200000\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 4*maxlen # Nucleotide data input (img shape: 4*561)\n",
    "dropout = 0.5 # Dropout, probability to keep units\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None,1])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(13561), Dimension(1), Dimension(564), Dimension(4)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape [#batches, height (1), # nucleotides, depth]\n",
    "x0 = tf.reshape(encoded_train,shape=[-1,1,maxlen,4])\n",
    "x0.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 4, 5)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape: [height (1), width, depth (4), number of convs]\n",
    "W0 = np.zeros((1,3,4,5),dtype=np.float32)\n",
    "W0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(13561), Dimension(1), Dimension(564), Dimension(5)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape: [# batches, height, width, #conv]\n",
    "conv_out = tf.nn.conv2d(x0,W0,strides=[1,1,1,1],padding='SAME')\n",
    "conv_out.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'MaxPool:0' shape=(13561, 1, 52, 5) dtype=float32>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.max_pool(conv_out,ksize=[1,1,11,1],strides=[1,1,11,1],padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### conv2d:\n",
    "* Input x: Tensor [# samples, 1 (height), #nucleotides, 4(channels)]\n",
    "* Convolution Matrix W: Tensor [1 (height), conv width, 4(channels), #convolutions(1)]\n",
    "* Strides: [1,1,stride width, 1]\n",
    "\n",
    "#### max_pool\n",
    "* Input x: [1, 1 (height), pool width, #convolutions(1)]\n",
    "* Pool size ksize: [1,1,pool width, 1]\n",
    "* Stride size strides: [1,1,pool width, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create some wrappers for simplicity\n",
    "def conv1d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, 1, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool1d(x, k=2):\n",
    "    # MaxPool1D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, 1, k, 1], strides=[1, 1, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Reshape input picture\n",
    "    x = tf.reshape(x, shape=[-1, 1, maxlen, 4])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv1d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool1d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv1d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool1d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    # 1x15 conv, 4 inputs, 32 outputs\n",
    "    'wc1': tf.Variable(tf.random_normal([1, 15, 4, 32])),\n",
    "    # 1x15 conv, 32 inputs, 64 outputs\n",
    "    'wc2': tf.Variable(tf.random_normal([1, 15, 32, 64])),\n",
    "    # fully connected, 564/4*64 inputs, 1024 outputs\n",
    "    'wd1': tf.Variable(tf.random_normal([int(564/4)*64, 1024])),\n",
    "    # 1024 inputs, 1 output\n",
    "    'out': tf.Variable(tf.random_normal([1024,1]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1]))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = conv_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.greater(pred,0),tf.greater(y,0))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_data = tf.constant(data.train.data)\n",
    "input_target = tf.constant(data.train.target)\n",
    "slice_x, slice_y = tf.train.slice_input_producer([input_data, input_target])\n",
    "prebatch_x, prebatch_y = tf.train.batch([slice_x, slice_y], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, Minibatch Loss= 1337338.250000, Training Accuracy= 0.59375\n",
      "Iter 2560, Minibatch Loss= 1305518.500000, Training Accuracy= 0.60156\n",
      "Iter 3840, Minibatch Loss= 1891828.000000, Training Accuracy= 0.46875\n",
      "Iter 5120, Minibatch Loss= 2059806.125000, Training Accuracy= 0.48438\n",
      "Iter 6400, Minibatch Loss= 2218297.500000, Training Accuracy= 0.44531\n",
      "Iter 7680, Minibatch Loss= 1599985.250000, Training Accuracy= 0.50781\n",
      "Iter 8960, Minibatch Loss= 1494785.500000, Training Accuracy= 0.53125\n",
      "Iter 10240, Minibatch Loss= 1510159.000000, Training Accuracy= 0.57031\n",
      "Iter 11520, Minibatch Loss= 1754207.250000, Training Accuracy= 0.43750\n",
      "Iter 12800, Minibatch Loss= 1528396.375000, Training Accuracy= 0.48438\n",
      "Iter 14080, Minibatch Loss= 2033380.375000, Training Accuracy= 0.50000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9fc6e07a022c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;31m# Run optimization op (backprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n\u001b[0;32m---> 14\u001b[0;31m                                        keep_prob: dropout})\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdisplay_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;31m# Calculate batch loss and accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anand/.virtualenvs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 710\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    711\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anand/.virtualenvs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 908\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anand/.virtualenvs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 958\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    959\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/anand/.virtualenvs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    963\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 965\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/anand/.virtualenvs/tensorflow/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(init)\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_x = sess.run(prebatch_x)\n",
    "        batch_y = sess.run(prebatch_y)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "                                                              y: batch_y,\n",
    "                                                              keep_prob: 1.})\n",
    "            print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc))\n",
    "        step += 1\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: data.test.data[:256],\n",
    "                                      y: data.test.target[:256],\n",
    "                                      keep_prob: 1.}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Create the graph, etc.\n",
    "# init_op = tf.initialize_all_variables()\n",
    "\n",
    "# # Create a session for running operations in the Graph.\n",
    "# sess = tf.Session()\n",
    "\n",
    "# # Initialize the variables (like the epoch counter).\n",
    "# sess.run(init_op)\n",
    "\n",
    "# # Start input enqueue threads.\n",
    "# coord = tf.train.Coordinator()\n",
    "# threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "# try:\n",
    "#     while not coord.should_stop():\n",
    "#         # Create training batch\n",
    "#         batch_x, batch_y = tf.train.batch([slice_x, slice_y], batch_size=batch_size)\n",
    "#         batch_x = sess.run(batch_x)\n",
    "#         batch_y = sess.run(batch_y)\n",
    "#         # Run optimization op (backprop)\n",
    "#         sess.run(optimizer, feed_dict={x: batch_x, y: batch_y,\n",
    "#                                        keep_prob: dropout})\n",
    "#         if step % display_step == 0:\n",
    "#             # Calculate batch loss and accuracy\n",
    "#             loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_x,\n",
    "#                                                               y: batch_y,\n",
    "#                                                               keep_prob: 1.})\n",
    "#             print(\"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "#                   \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "#                   \"{:.5f}\".format(acc))\n",
    "\n",
    "# except tf.errors.OutOfRangeError:\n",
    "#     print('Done training -- epoch limit reached')\n",
    "# finally:\n",
    "#     # When done, ask the threads to stop.\n",
    "#     coord.request_stop()\n",
    "\n",
    "# # Wait for threads to finish.\n",
    "# coord.join(threads)\n",
    "# sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
